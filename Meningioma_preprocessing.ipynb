{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing brain MRI of patients with meningioma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before using MONSTR\n",
    "\n",
    "### Setting PATH variable\n",
    "**The environment PATH MUST be set in the main terminal, not otherwise like in a sub-terminal in the Jupyterlab**\n",
    "\n",
    "#### ANTs\n",
    "export ANTSPATH=/opt/ANTs/bin/\n",
    "\n",
    "export PATH=${ANTSPATH}:$PATH\n",
    "\n",
    "#### FSL\n",
    "FSLDIR=/usr/local/fsl\n",
    ". ${FSLDIR}/etc/fslconf/fsl.sh\n",
    "\n",
    "PATH=${FSLDIR}/bin:${PATH}\n",
    "\n",
    "export FSLDIR PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update\n",
    "6/22 \\\n",
    "Added : Code that applies brain mask to MR images.\n",
    "\n",
    "6/23 \\\n",
    "Added: New variable T1C_label added to co-registration and skull-stripping steps. \\\n",
    "Now users can put T1C name by changing variable 'T1C_label' (\"T1GD\", \"T1CE\", \"T1C\", \"BB\" etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dicom\\__init__.py:53: UserWarning: \n",
      "This code is using an older version of pydicom, which is no longer \n",
      "maintained as of Jan 2017.  You can access the new pydicom features and API \n",
      "by installing `pydicom` from PyPI.\n",
      "See 'Transitioning to pydicom 1.x' section at pydicom.readthedocs.org \n",
      "for more information.\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(path_lst):\n",
    "    for path in path_lst:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# make outputdir and return output path (including filename) by combining outDir, pt, filename\n",
    "def get_outpath(outDir, pt, filename):\n",
    "    outPath = os.path.join(outDir, pt, filename)\n",
    "\n",
    "    make_dirs([os.path.join(outDir, pt)])\n",
    "\n",
    "    if os.path.exists(outPath):\n",
    "        print('{} already exists'.format(filename))\n",
    "        return None\n",
    "    else:\n",
    "        return outPath\n",
    "\n",
    "# find sequence from filename\n",
    "def find_seq(filename):\n",
    "    seq_lst = [\"T1GD\", \"T1CE\", \"T1C\", \"T1\", \"T2\", \"FLAIR\", \"FLAIRGD\", \"ADC\", \"BB\", \"WB\"]\n",
    "    seq = None\n",
    "    for i in seq_lst:\n",
    "        if i in filename.upper():\n",
    "            seq = i\n",
    "            return seq\n",
    "    if seq is None:\n",
    "        print(\"No matching sequence\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceDir = '/media/scmlsg/DATA2/20200616_Brain_metastasis_test_set_nii'\n",
    "targetDir = '/media/scmlsg/DATA2/20200616_Brain_metastasis_test_set_nii_preprocessed'\n",
    "outputPath = '/home/scmlsg/MONSTR/output'\n",
    "\n",
    "resampleDir_mask = os.path.join(targetDir, \"Tumor_masks_resampled\")\n",
    "resampleDir = os.path.join(targetDir, \"Resampled\")\n",
    "correctionDir = os.path.join(targetDir, \"Corrected\")\n",
    "registerDir = os.path.join(targetDir, 'Registered')\n",
    "brainmaskDir = os.path.join(targetDir, 'BrainMask')\n",
    "strippedDir = os.path.join(targetDir, 'Stripped_wo_Normalized')\n",
    "\n",
    "make_dirs([targetDir, resampleDir, correctionDir, registerDir, brainmaskDir, strippedDir])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check directory structure\n",
    "for (path, dirs, files) in os.walk(sourceDir):\n",
    "    print(path, dirs, files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling: #1, 10020976_BB.nii\n",
      "Resampling: #2, 10020976_T2.nii\n",
      "Resampling: #3, 10020976_WB.nii\n",
      "Resampling: #4, 10033133_BB.nii\n",
      "Resampling: #5, 10033133_T2.nii\n",
      "Resampling: #6, 10033133_WB.nii\n",
      "Resampling: #7, 10034339_BB.nii\n",
      "Resampling: #8, 10034339_T2.nii\n",
      "Resampling: #9, 10034339_WB.nii\n",
      "Resampling: #10, 10064077_BB.nii\n",
      "Resampling: #11, 10064077_T2.nii\n",
      "Resampling: #12, 10064077_WB.nii\n",
      "Resampling: #13, 10110484_BB.nii\n",
      "Resampling: #14, 10110484_T2.nii\n",
      "Resampling: #15, 10110484_WB.nii\n",
      "Resampling: #16, 10137211_BB.nii\n",
      "Resampling: #17, 10137211_T2.nii\n",
      "Resampling: #18, 10137211_WB.nii\n",
      "Resampling: #19, 10165247_BB.nii\n",
      "Resampling: #20, 10165247_T2.nii\n",
      "Resampling: #21, 10165247_WB.nii\n",
      "Resampling: #22, 10168781_BB.nii\n",
      "Resampling: #23, 10168781_T2.nii\n",
      "Resampling: #24, 10168781_WB.nii\n",
      "Resampling: #25, 10174199_BB.nii\n",
      "Resampling: #26, 10174199_T2.nii\n",
      "Resampling: #27, 10174199_WB.nii\n",
      "Resampling: #28, 10177937_BB.nii\n",
      "Resampling: #29, 10177937_T2.nii\n",
      "Resampling: #30, 10177937_WB.nii\n",
      "Resampling: #31, 10182376_BB.nii\n",
      "Resampling: #32, 10182376_T2.nii\n",
      "Resampling: #33, 10182376_WB.nii\n",
      "Resampling: #34, 10183313_BB.nii\n",
      "Resampling: #35, 10183313_T2.nii\n",
      "Resampling: #36, 10183313_WB.nii\n",
      "Resampling: #37, 10198431_BB.nii\n",
      "Resampling: #38, 10198431_T2.nii\n",
      "Resampling: #39, 10198431_WB.nii\n",
      "Resampling: #40, 10207110_BB.nii\n",
      "Resampling: #41, 10207110_T2.nii\n",
      "Resampling: #42, 10207110_WB.nii\n",
      "Resampling: #43, 10208301_BB.nii\n",
      "Resampling: #44, 10208301_T2.nii\n",
      "Resampling: #45, 10208301_WB.nii\n",
      "Resampling: #46, 2089794_BB.nii\n",
      "Resampling: #47, 2089794_T2.nii\n",
      "Resampling: #48, 2089794_WB.nii\n",
      "Resampling: #49, 2244819_BB.nii\n",
      "Resampling: #50, 2244819_T2.nii\n",
      "Resampling: #51, 2244819_WB.nii\n",
      "Resampling: #52, 3219808_BB.nii\n",
      "Resampling: #53, 3219808_T2.nii\n",
      "Resampling: #54, 3219808_WB.nii\n",
      "Resampling: #55, 3902261_BB.nii\n",
      "Resampling: #56, 3902261_T2.nii\n",
      "Resampling: #57, 3902261_WB.nii\n",
      "Resampling: #58, 3940812_BB.nii\n",
      "Resampling: #59, 3940812_T2.nii\n",
      "Resampling: #60, 3940812_WB.nii\n",
      "Resampling: #61, 5652785_BB.nii\n",
      "Resampling: #62, 5652785_T2.nii\n",
      "Resampling: #63, 5652785_WB.nii\n",
      "Resampling: #64, 5659286_BB.nii\n",
      "Resampling: #65, 5659286_T2.nii\n",
      "Resampling: #66, 5659286_WB.nii\n",
      "Resampling: #67, 5802208_BB.nii\n",
      "Resampling: #68, 5802208_T2.nii\n",
      "Resampling: #69, 5802208_WB.nii\n",
      "Resampling: #70, 5810019_BB.nii\n",
      "Resampling: #71, 5810019_T2.nii\n",
      "Resampling: #72, 5810019_WB.nii\n",
      "Resampling: #73, 7135279_BB.nii\n",
      "Resampling: #74, 7135279_T2.nii\n",
      "Resampling: #75, 7135279_WB.nii\n",
      "Resampling: #76, 7579610_BB.nii\n",
      "Resampling: #77, 7579610_T2.nii\n",
      "Resampling: #78, 7579610_WB.nii\n",
      "Resampling: #79, 8419228_BB.nii\n",
      "Resampling: #80, 8419228_T2.nii\n",
      "Resampling: #81, 8419228_WB.nii\n",
      "Resampling: #82, 8543740_BB.nii\n",
      "Resampling: #83, 8543740_T2.nii\n",
      "Resampling: #84, 8543740_WB.nii\n",
      "Resampling: #85, 8608440_BB.nii\n",
      "Resampling: #86, 8608440_T2.nii\n",
      "Resampling: #87, 8608440_WB.nii\n",
      "Resampling: #88, 8761757_BB.nii\n",
      "Resampling: #89, 8761757_T2.nii\n",
      "Resampling: #90, 8761757_WB.nii\n",
      "Resampling: #91, 8852237_BB.nii\n",
      "Resampling: #92, 8852237_T2.nii\n",
      "Resampling: #93, 8852237_WB.nii\n"
     ]
    }
   ],
   "source": [
    "# 1) resampling\n",
    "total = 0\n",
    "for (path, dirs, files) in os.walk(sourceDir):\n",
    "    for file in files:\n",
    "        total += 1\n",
    "        print(\"Resampling: #{}, {}\".format(total, file))\n",
    "        filePath = os.path.join(path, file)\n",
    "        pt = file.split(\"_\")[0]\n",
    "        if \"-label\" in file:\n",
    "            resample_outPath = get_outpath(resampleDir_mask, pt, file)\n",
    "        else:\n",
    "            resample_outPath = get_outpath(resampleDir, pt, file)\n",
    "\n",
    "        if resample_outPath is None:\n",
    "            continue\n",
    "        !ResampleImage 3 {filePath} {resample_outPath} 1x1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N4 bias correction: #1, 10020976_BB.nii\n",
      "N4 bias correction: #2, 10020976_T2.nii\n",
      "N4 bias correction: #3, 10020976_WB.nii\n",
      "N4 bias correction: #4, 10033133_BB.nii\n",
      "N4 bias correction: #5, 10033133_T2.nii\n",
      "N4 bias correction: #6, 10033133_WB.nii\n",
      "N4 bias correction: #7, 10034339_BB.nii\n",
      "N4 bias correction: #8, 10034339_T2.nii\n",
      "N4 bias correction: #9, 10034339_WB.nii\n",
      "N4 bias correction: #10, 10064077_BB.nii\n",
      "N4 bias correction: #11, 10064077_T2.nii\n",
      "N4 bias correction: #12, 10064077_WB.nii\n",
      "N4 bias correction: #13, 10110484_BB.nii\n",
      "N4 bias correction: #14, 10110484_T2.nii\n",
      "N4 bias correction: #15, 10110484_WB.nii\n",
      "N4 bias correction: #16, 10137211_BB.nii\n",
      "N4 bias correction: #17, 10137211_T2.nii\n",
      "N4 bias correction: #18, 10137211_WB.nii\n",
      "N4 bias correction: #19, 10165247_BB.nii\n",
      "N4 bias correction: #20, 10165247_T2.nii\n",
      "N4 bias correction: #21, 10165247_WB.nii\n",
      "N4 bias correction: #22, 10168781_BB.nii\n",
      "N4 bias correction: #23, 10168781_T2.nii\n",
      "N4 bias correction: #24, 10168781_WB.nii\n",
      "N4 bias correction: #25, 10174199_BB.nii\n",
      "N4 bias correction: #26, 10174199_T2.nii\n",
      "N4 bias correction: #27, 10174199_WB.nii\n",
      "N4 bias correction: #28, 10177937_BB.nii\n",
      "N4 bias correction: #29, 10177937_T2.nii\n",
      "N4 bias correction: #30, 10177937_WB.nii\n",
      "N4 bias correction: #31, 10182376_BB.nii\n",
      "N4 bias correction: #32, 10182376_T2.nii\n",
      "N4 bias correction: #33, 10182376_WB.nii\n",
      "N4 bias correction: #34, 10183313_BB.nii\n",
      "N4 bias correction: #35, 10183313_T2.nii\n",
      "N4 bias correction: #36, 10183313_WB.nii\n",
      "N4 bias correction: #37, 10198431_BB.nii\n",
      "N4 bias correction: #38, 10198431_T2.nii\n",
      "N4 bias correction: #39, 10198431_WB.nii\n",
      "N4 bias correction: #40, 10207110_BB.nii\n",
      "N4 bias correction: #41, 10207110_T2.nii\n",
      "N4 bias correction: #42, 10207110_WB.nii\n",
      "N4 bias correction: #43, 10208301_BB.nii\n",
      "N4 bias correction: #44, 10208301_T2.nii\n",
      "N4 bias correction: #45, 10208301_WB.nii\n",
      "N4 bias correction: #46, 2089794_BB.nii\n",
      "N4 bias correction: #47, 2089794_T2.nii\n",
      "N4 bias correction: #48, 2089794_WB.nii\n",
      "N4 bias correction: #49, 2244819_BB.nii\n",
      "N4 bias correction: #50, 2244819_T2.nii\n",
      "N4 bias correction: #51, 2244819_WB.nii\n",
      "N4 bias correction: #52, 3219808_BB.nii\n",
      "N4 bias correction: #53, 3219808_T2.nii\n",
      "N4 bias correction: #54, 3219808_WB.nii\n",
      "N4 bias correction: #55, 3902261_BB.nii\n",
      "N4 bias correction: #56, 3902261_T2.nii\n",
      "N4 bias correction: #57, 3902261_WB.nii\n",
      "N4 bias correction: #58, 3940812_BB.nii\n",
      "N4 bias correction: #59, 3940812_T2.nii\n",
      "N4 bias correction: #60, 3940812_WB.nii\n",
      "N4 bias correction: #61, 5652785_BB.nii\n",
      "N4 bias correction: #62, 5652785_T2.nii\n",
      "N4 bias correction: #63, 5652785_WB.nii\n",
      "N4 bias correction: #64, 5659286_BB.nii\n",
      "N4 bias correction: #65, 5659286_T2.nii\n",
      "N4 bias correction: #66, 5659286_WB.nii\n",
      "N4 bias correction: #67, 5802208_BB.nii\n",
      "N4 bias correction: #68, 5802208_T2.nii\n",
      "N4 bias correction: #69, 5802208_WB.nii\n",
      "N4 bias correction: #70, 5810019_BB.nii\n",
      "N4 bias correction: #71, 5810019_T2.nii\n",
      "N4 bias correction: #72, 5810019_WB.nii\n",
      "N4 bias correction: #73, 7135279_BB.nii\n",
      "N4 bias correction: #74, 7135279_T2.nii\n",
      "N4 bias correction: #75, 7135279_WB.nii\n",
      "N4 bias correction: #76, 7579610_BB.nii\n",
      "N4 bias correction: #77, 7579610_T2.nii\n",
      "N4 bias correction: #78, 7579610_WB.nii\n",
      "N4 bias correction: #79, 8419228_BB.nii\n",
      "N4 bias correction: #80, 8419228_T2.nii\n",
      "N4 bias correction: #81, 8419228_WB.nii\n",
      "N4 bias correction: #82, 8543740_BB.nii\n",
      "N4 bias correction: #83, 8543740_T2.nii\n",
      "N4 bias correction: #84, 8543740_WB.nii\n",
      "N4 bias correction: #85, 8608440_BB.nii\n",
      "N4 bias correction: #86, 8608440_T2.nii\n",
      "N4 bias correction: #87, 8608440_WB.nii\n",
      "N4 bias correction: #88, 8761757_BB.nii\n",
      "N4 bias correction: #89, 8761757_T2.nii\n",
      "N4 bias correction: #90, 8761757_WB.nii\n",
      "N4 bias correction: #91, 8852237_BB.nii\n",
      "N4 bias correction: #92, 8852237_T2.nii\n",
      "N4 bias correction: #93, 8852237_WB.nii\n"
     ]
    }
   ],
   "source": [
    "# 2) N4 bias correction\n",
    "total = 0\n",
    "for (path, dirs, files) in os.walk(resampleDir):\n",
    "    for file in files:\n",
    "        total += 1\n",
    "        print(\"N4 bias correction: #{}, {}\".format(total, file))\n",
    "        filePath = os.path.join(path, file)\n",
    "        pt = file.split(\"_\")[0]\n",
    "        corr_outPath = get_outpath(correctionDir, pt, file)\n",
    "        if corr_outPath is None:\n",
    "            continue\n",
    "        !N4BiasFieldCorrection -d 3 -i {filePath} -o {corr_outPath} -s 3 -c [50x50x50x50,0.00001] -b [150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coregistration: #1, ['10020976_BB.nii', '10020976_T2.nii', '10020976_WB.nii']\n",
      "10020976_BB_registered.nii already exists\n",
      "Registering T2 to BB...\n",
      "10020976_T2_registered.nii already exists\n",
      "Registering WB to BB...\n",
      "10020976_WB_registered.nii already exists\n",
      "coregistration: #2, ['10033133_BB.nii', '10033133_T2.nii', '10033133_WB.nii']\n",
      "10033133_BB_registered.nii already exists\n",
      "Registering T2 to BB...\n",
      "10033133_T2_registered.nii already exists\n",
      "Registering WB to BB...\n",
      "10033133_WB_registered.nii already exists\n",
      "coregistration: #3, ['10034339_BB.nii', '10034339_T2.nii', '10034339_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #4, ['10064077_BB.nii', '10064077_T2.nii', '10064077_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #5, ['10110484_BB.nii', '10110484_T2.nii', '10110484_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #6, ['10137211_BB.nii', '10137211_T2.nii', '10137211_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #7, ['10165247_BB.nii', '10165247_T2.nii', '10165247_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #8, ['10168781_BB.nii', '10168781_T2.nii', '10168781_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #9, ['10174199_BB.nii', '10174199_T2.nii', '10174199_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #10, ['10177937_BB.nii', '10177937_T2.nii', '10177937_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #11, ['10182376_BB.nii', '10182376_T2.nii', '10182376_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #12, ['10183313_BB.nii', '10183313_T2.nii', '10183313_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #13, ['10198431_BB.nii', '10198431_T2.nii', '10198431_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #14, ['10207110_BB.nii', '10207110_T2.nii', '10207110_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #15, ['10208301_BB.nii', '10208301_T2.nii', '10208301_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #16, ['2089794_BB.nii', '2089794_T2.nii', '2089794_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #17, ['2244819_BB.nii', '2244819_T2.nii', '2244819_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #18, ['3219808_BB.nii', '3219808_T2.nii', '3219808_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n",
      "coregistration: #19, ['3902261_BB.nii', '3902261_T2.nii', '3902261_WB.nii']\n",
      "Registering T2 to BB...\n",
      "Registering WB to BB...\n"
     ]
    }
   ],
   "source": [
    "# 3) co-registration\n",
    "T1C_label = 'BB' # 'BB','T1C','T1GD' etc.\n",
    "\n",
    "total = 0\n",
    "for (path, dirs, files) in os.walk(correctionDir):\n",
    "    if files != []:\n",
    "        total += 1\n",
    "        print(\"coregistration: #{}, {}\".format(total, files))\n",
    "        pt = path.split(os.path.sep)[-1]\n",
    "\n",
    "        T1C_path = glob(os.path.join(path, '*{}*'.format(T1C_label)))[0]\n",
    "        T1C_regist_outPath = get_outpath(registerDir, pt, pt + '_{}_registered.nii'.format(T1C_label))\n",
    "        if os.path.splitext(T1C_path)[-1] == '.gz':\n",
    "            T1C_regist_outPath = get_outpath(registerDir, pt, pt + '_{}_registered.nii.gz'.format(T1C_label))\n",
    "\n",
    "        if T1C_regist_outPath is not None:\n",
    "            shutil.copyfile(T1C_path, T1C_regist_outPath)\n",
    "        \n",
    "        for file in files:\n",
    "            seq = find_seq(file)\n",
    "            if seq == T1C_label:\n",
    "                continue\n",
    "            in_path = os.path.join(path, file)\n",
    "            print(\"Registering {} to {}...\".format(seq, T1C_label))\n",
    "            outPath = get_outpath(registerDir, pt, pt + '_{}_registered.nii'.format(seq))\n",
    "            if outPath is None:\n",
    "                continue\n",
    "\n",
    "            !antsRegistration -d 3 -r [{T1C_path}, {in_path}, 1] \\\n",
    "                              -m mattes[{T1C_path}, {in_path}, 1, 32, regular, 0.1 ] \\\n",
    "                              -t translation[0.1] -c [100x50x25,1.e-8,20] -s 4x2x1vox -f 6x4x2 -l 1 \\\n",
    "                              -m mattes[{T1C_path}, {in_path} , 1 , 32, regular, 0.1] \\\n",
    "                              -t rigid[0.1] -c [100x50x25,1.e-8,20]  -s 4x2x1vox  -f 3x2x1 -l 1 -o [reg, {outPath}] -v 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4) Skull stripping using MONSTR\n",
    "\n",
    "# we'll work in the MONSTR directory\n",
    "workingDir = '/home/scmlsg/MONSTR'\n",
    "%cd {workingDir} \n",
    "outputPath = '/home/scmlsg/MONSTR/output'\n",
    "\n",
    "T1C_label = 'BB' # 'BB','T1C','T1GD' etc.\n",
    "total = 0\n",
    "\n",
    "for (path, dirs, files) in os.walk(registerDir):\n",
    "    if files != []:\n",
    "        total += 1\n",
    "        print(\"skull stripping: #{}, {}\".format(total, files))\n",
    "\n",
    "        pt = path.split(os.path.sep)[-1]\n",
    "        T1C_path = glob(os.path.join(path, '*{}*'.format(T1C_label)))[0]\n",
    "        T2_path = glob(os.path.join(path, '*T2*'))[0]\n",
    "        newFileName = pt + '_brain_mask.nii.gz'\n",
    "        if os.path.exists(os.path.join(brainmaskDir, newFileName)):\n",
    "            print(\"{} already exists\".format(newFileName))\n",
    "            continue\n",
    "\n",
    "        !./MONSTR.sh --t1 {T1C_path} --t2 {T2_path} --o output --atlasdir TBI_atlas --reg\n",
    "\n",
    "        createdDir = os.path.join(outputPath, os.listdir(outputPath)[0])\n",
    "\n",
    "        for file in os.listdir(createdDir):\n",
    "            if file == 'atlas4_brainmask_ANTS.nii.gz':\n",
    "                shutil.copyfile(os.path.join(createdDir, file), os.path.join(brainmaskDir, newFileName))\n",
    "\n",
    "        shutil.rmtree(createdDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5) apply skull mask\n",
    "total = 0\n",
    "error_dict = {}\n",
    "\n",
    "for (path, dirs, files) in os.walk(registerDir):\n",
    "    if files != []:\n",
    "        total += 1\n",
    "        pt = path.split(os.path.sep)[-1]\n",
    "        maskName = pt + '_brain_mask.nii.gz'\n",
    "        mask = nib.load(os.path.join(brainmaskDir, maskName))\n",
    "\n",
    "        for file in files:\n",
    "            seq = find_seq(file)\n",
    "            print(\"# {} Applying skull mask to {}...\".format(total, file))\n",
    "\n",
    "            # Reshape (in some cases data resolution is (x,y,z,1), reshape removes the 4th dimension\n",
    "            file_nii = nib.load(os.path.join(path, file))\n",
    "            file_data = file_nii.get_data()\n",
    "            file_data = file_data.reshape(file_data.shape[0], file_data.shape[1], file_data.shape[2])\n",
    "\n",
    "            outPath = get_outpath(strippedDir, pt, pt + \"_{}_stripped.nii.gz\".format(seq))\n",
    "            if outPath is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                masked_data = np.multiply(file_data, mask.get_data())\n",
    "                masked_data = nib.Nifti1Image(masked_data, file_nii.affine, file_nii.header)\n",
    "                nib.save(masked_data, outPath)\n",
    "            except Exception as ex:\n",
    "                print(\"Error:{}, filename:{}\".format(ex.args, file))\n",
    "                error_dict[pt] = ex.args\n",
    "                pass\n",
    "\n",
    "print(\"Finished!\")\n",
    "print(error_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## below are previous codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcDir = \n",
    "    print(\"N4 bias correction: \", pt)\n",
    "    correctionDir = os.path.join(targetDir, \"correction\")\n",
    "    \n",
    "    if not os.path.exists(correctionDir):\n",
    "        os.makedirs(correctionDir)\n",
    "                          \n",
    "    for file in os.listdir(resampleDir):\n",
    "        filePath = os.path.join(resampleDir, pt, file)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(correctionDir, pt)):\n",
    "            os.makedirs(os.path.join(correctionDir, pt))\n",
    "            \n",
    "        outPath = os.path.join(correctionDir, pt, file)\n",
    "        #!ResampleImage 3 {filePath} {outPath} 1x1x1\n",
    "        print(filePath, outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (BB, T2, WB) in zip(T1_BB, T2, T1_WB):\n",
    "    # patient ID number (MRN) is extracted as a variable 'mrn'\n",
    "    mrn = BB.split('_')[0] \n",
    "    print((BB, T2, WB))\n",
    "     print(\"<<< Started working on the following case: \", mrn, \" >>>\")\n",
    "        \n",
    "    # 0. Copy the current T1 and T2 files onto the working directory \n",
    "    #    (to avoid 'sudo' problem in trying to write a file on an external storage)\n",
    "    pathList = []\n",
    "    for file in os.listdir(patientDir):\n",
    "        shutil.copyfile(os.path.join(patientDir, file), os.path.join(workingDir, file))\n",
    "        pathList.append(os.path.join(workingDir, file))\n",
    "    \n",
    "    # 1. Resampling onto 1mm x 1mm x 1mm images \n",
    "    print(\"1) Resampling to 1-mm isovoxel images: \", end=\"\")\n",
    "    for file in pathList:\n",
    "        !ResampleImage 3 {file} {file} 1x1x1\n",
    "        print(file, end=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "##########metastasis for PYW ###########3\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# we'll work in the MONSTR directory\n",
    "workingDir = '/home/chansik/MONSTR'\n",
    "%cd {workingDir} \n",
    "\n",
    "# set paths for source and target\n",
    "sourceDir = '/media/chansik/Samsung_T5/mets_nifti'\n",
    "targetDir = '/media/chansik/Samsung_T5/mets_stripped'\n",
    "outputPath = '/home/chansik/MONSTR/output'\n",
    "##########################################################################################\n",
    "\n",
    "alreadyDoneList = []\n",
    "\n",
    "# file names MUST be BB.nii, WB.nii, T2.nii, FLAIR.nii, and ADC.nii\n",
    "\n",
    "for patient in os.listdir(sourceDir):\n",
    "    patientDir = os.path.join(sourceDir, patient)\n",
    "    \n",
    "    # check if preprocessing has already been done\n",
    "    # if so, skip the patient and move onto the next patient\n",
    "    if patient in os.listdir(targetDir):\n",
    "        print(patient, 'has already been processed!')\n",
    "        alreadyDoneList.append(patient)\n",
    "        continue\n",
    "    \n",
    "    print(\"<<< Started working on the following case: \", patient, \" >>>\")\n",
    "    \n",
    "    # 0. Copy the current T1 and T2 files onto the working directory \n",
    "    #    (to avoid 'sudo' problem in trying to write a file on an external storage)\n",
    "    pathList = []\n",
    "    for file in os.listdir(patientDir):\n",
    "        shutil.copyfile(os.path.join(patientDir, file), os.path.join(workingDir, file))\n",
    "        pathList.append(os.path.join(workingDir, file))\n",
    "    \n",
    "    # 1. Resampling onto 1mm x 1mm x 1mm images \n",
    "    print(\"1) Resampling to 1-mm isovoxel images: \", end=\"\")\n",
    "    for file in pathList:\n",
    "        !ResampleImage 3 {file} {file} 1x1x1\n",
    "        print(file, end=\"  \")\n",
    "    \n",
    "    # 2. N4 bias correction\n",
    "    # by using ANTs, using the same parameters as the default one used by MONSTR\n",
    "    print(\"\\n2) N4 bias field correction: \", end=\"\")\n",
    "    for file in pathList:\n",
    "        !N4BiasFieldCorrection -d 3 -i {file} -o {file} -s 3 -c [50x50x50x50,0.00001] -b [150]\n",
    "        print(file, end=\"  \")\n",
    "  \n",
    "    # 3. Co-registration\n",
    "    # by using ANTs, using the same parameters as the default one used by MONSTR\n",
    "    print(\"\\n3) Co-registration with CE-T1 images as fixed, reference images: \", end=\"\")\n",
    "    for file in [\"T2.nii\", \"WB.nii\"]:\n",
    "        newName = file.split(\".\")[0] + \"_registered.nii\"\n",
    "        !antsRegistration -d 3 -r [BB.nii, {file}, 1] \\\n",
    "                          -m mattes[BB.nii, {file}, 1, 32, regular, 0.1 ] \\\n",
    "                          -t translation[0.1] -c [100x50x25,1.e-8,20] -s 4x2x1vox -f 6x4x2 -l 1 \\\n",
    "                          -m mattes[BB.nii, {file} , 1 , 32, regular, 0.1] \\\n",
    "                          -t rigid[0.1] -c [100x50x25,1.e-8,20]  -s 4x2x1vox  -f 3x2x1 -l 1 -o [reg, {newName}] -v 0\n",
    "        print(file, end=\"  \")\n",
    "    shutil.copyfile(\"BB.nii\", \"BB_registered.nii\")\n",
    " \n",
    "    # 4. MONSTR skull stripping\n",
    "    t1 = \"WB_registered.nii\"\n",
    "    t2 = \"T2_registered.nii\"\n",
    "    !./MONSTR.sh --t1 {t1} --t2 {t2} --o output --atlasdir TBI_atlas --reg\n",
    "\n",
    "    # Copy the relevant files (brain Mask and processed images)\n",
    "    createdDir = os.path.join(outputPath, os.listdir(outputPath)[0])\n",
    "\n",
    "    os.makedirs(os.path.join(targetDir, patient))\n",
    "    for file in os.listdir(createdDir):\n",
    "        if file == 'atlas4_brainmask_ANTS.nii.gz':\n",
    "            newFileName = patient + '/brain_mask.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "    shutil.copyfile(\"WB_registered.nii\", os.path.join(targetDir, patient, \"WB_registered.nii\"))\n",
    "    shutil.copyfile(\"BB_registered.nii\", os.path.join(targetDir, patient, \"BB_registered.nii\"))\n",
    "    shutil.copyfile(\"T2_registered.nii\", os.path.join(targetDir, patient, \"T2_registered.nii\"))\n",
    " \n",
    "    # 5. Clean the temporarily created files and directories\n",
    "    shutil.rmtree(createdDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceDir = '/media/chansik/Samsung_T5/test'\n",
    "targetDir = '/media/chansik/Samsung_T5/mets_stripped'\n",
    "outputPath = '/home/chansik/MONSTR/output'\n",
    "\n",
    "# Copy the relevant files (brain Mask and processed images)\n",
    "createdDir = os.path.join(outputPath, os.listdir(outputPath)[0])\n",
    "patient = '1234'\n",
    "\n",
    "os.makedirs(os.path.join(targetDir, patient))\n",
    "for file in os.listdir(createdDir):\n",
    "    if file == 'atlas4_brainmask_ANTS.nii.gz':\n",
    "        newFileName = patient + '/brain_mask.nii.gz'\n",
    "        shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "    \n",
    "shutil.copyfile(\"WB_registered.nii\", os.path.join(targetDir, patient, \"WB_registered.nii\"))\n",
    "shutil.copyfile(\"BB_registered.nii\", os.path.join(targetDir, patient, \"BB_registered.nii\"))\n",
    "shutil.copyfile(\"T2_registered.nii\", os.path.join(targetDir, patient, \"T2_registered.nii\"))\n",
    "\n",
    "# Clean the temporarily created files and directories\n",
    "#os.remove(os.path.join(workingDir, t1))\n",
    "#os.remove(os.path.join(workingDir, t2))\n",
    "#shutil.rmtree(createdDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"BB_registered.nii\"\n",
    "t2 = \"T2_registered.nii\"\n",
    "\n",
    "!./MONSTR.sh --t1 {t1} --t2 {t2} --o output --atlasdir TBI_atlas --reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !./MONSTR.sh --t1 {t1} --t2 {t2} --o output --atlasdir TBI_atlas --reg\n",
    "\n",
    "    # 3. Copy the relevant files (brain Mask and processed T1/T2 images)\n",
    "    createdDir = os.path.join(outputPath, os.listdir(outputPath)[0])\n",
    "\n",
    "    for file in os.listdir(createdDir):\n",
    "        if file == 'atlas4_brainmask_ANTS.nii.gz':\n",
    "            newFileName = mrn + '_bMask.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "        if file == 't1.nii.gz':\n",
    "            newFileName = mrn + '_T1GD_processed.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "        if file == 't2.nii.gz':\n",
    "            newFileName = mrn + '_T2_processed.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "    # 4. Clean the temporarily created files and directories\n",
    "    os.remove(os.path.join(workingDir, t1))\n",
    "    os.remove(os.path.join(workingDir, t2))\n",
    "    shutil.rmtree(createdDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "##########test ###########3\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# we'll work in the MONSTR directory\n",
    "workingDir = '/home/chansik/MONSTR'\n",
    "%cd {workingDir} \n",
    "\n",
    "# set paths for source and target\n",
    "sourceDir = '/media/chansik/Samsung_T5'\n",
    "targetDir = '/media/chansik/SC_research_2019/GBM-Surv/Seg_results'\n",
    "##########################################################################################\n",
    "\n",
    "alreadyDoneList = []\n",
    "\n",
    "# file names MUST be CE-T1.nii, UE-T1.nii, T2.nii, FLAIR.nii, and ADC.nii\n",
    "\n",
    "for patient in os.listdir(sourceDir):\n",
    "    patientDir = os.path.join(sourceDir, patient)\n",
    "    \n",
    "    # check if preprocessing has already been done\n",
    "    # if so, skip the patient and move onto the next patient\n",
    "    if patient in os.listdir(targetDir):\n",
    "        print(patient, 'has already been processed!')\n",
    "        alreadyDoneList.append(patient)\n",
    "        continue\n",
    "    \n",
    "    print(\"<<< Started working on the following case: \", patient, \" >>>\")\n",
    "    \n",
    "    # 0. Copy the current T1 and T2 files onto the working directory \n",
    "    #    (to avoid 'sudo' problem in trying to write a file on an external storage)\n",
    "    pathList = []\n",
    "    for file in os.listdir(patientDir):\n",
    "        shutil.copyfile(os.path.join(patientDir, file), os.path.join(workingDir, file))\n",
    "        pathList.append(os.path.join(workingDir, file))\n",
    "    \n",
    "    # 1. Resampling onto 1mm x 1mm x 1mm images \n",
    "    print(\"1) Resampling to 1-mm isovoxel images: \", end=\"\")\n",
    "    for file in pathList:\n",
    "        !ResampleImage 3 {file} {file} 1x1x1\n",
    "        print(file, end=\"  \")\n",
    "    \n",
    "    # 2. N4 bias correction\n",
    "    # by using ANTs, using the same parameters as the default one used by MONSTR\n",
    "    print(\"\\n2) N4 bias field correction: \", end=\"\")\n",
    "    for file in pathList:\n",
    "        !N4BiasFieldCorrection -d 3 -i {file} -o {file} -s 3 -c [50x50x50x50,0.00001] -b [150]\n",
    "        print(file, end=\"  \")\n",
    "  \n",
    "    # 3. Co-registration\n",
    "    # by using ANTs, using the same parameters as the default one used by MONSTR\n",
    "    print(\"\\n3) Co-registration with CE-T1 images as fixed, reference images: \", end=\"\")\n",
    "    for file in [\"UE-T1.nii\", \"T2.nii\", \"FLAIR.nii\", \"ADC.nii\"]:\n",
    "        newName = file.split(\".\")[0] + \"_registered.nii\"\n",
    "        !antsRegistration -d 3 -r [CE-T1.nii, {file}, 1] \\\n",
    "                          -m mattes[CE-T1.nii, {file}, 1, 32, regular, 0.1 ] \\\n",
    "                          -t translation[0.1] -c [100x50x25,1.e-8,20] -s 4x2x1vox -f 6x4x2 -l 1 \\\n",
    "                          -m mattes[CE-T1.nii, {file} , 1 , 32, regular, 0.1] \\\n",
    "                          -t rigid[0.1] -c [100x50x25,1.e-8,20]  -s 4x2x1vox  -f 3x2x1 -l 1 -o [reg, {newName}] -v 0\n",
    "        print(file, end=\"  \")\n",
    "    shutil.copyfile(\"CE-T1.nii\", \"CE-T1_registered.nii\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # check if preprocessing has already been done\n",
    "    # if so, skip the patient and move onto the next patient\n",
    "    if mrn in [i.split('_', 1)[0] for i in os.listdir(targetDir)]:\n",
    "        print(mrn, 'has already been processed!')\n",
    "        alreadyDoneList.append(mrn)\n",
    "        continue\n",
    "       \n",
    "    # 0. Copy the current T1 and T2 files onto the working directory \n",
    "    #    (to avoid 'sudo' problem in trying to write a file on an external storage)\n",
    "    shutil.copyfile(os.path.join(sourceDir, t1), os.path.join(workingDir, t1))\n",
    "    shutil.copyfile(os.path.join(sourceDir, t2), os.path.join(workingDir, t2))\n",
    "    \n",
    "    # 1. Resampling of CE-T1-weighted images onto 1mm x 1mm x 1mm images \n",
    "    #   (as some T1 images are 2D)\n",
    "    !ResampleImage 3 {t1} {t1} 1x1x1 \n",
    "    \n",
    "    # 2. Skull stripping by MONSTR algorithm following \n",
    "    #          1) rigid coregistration of T2 onto T1 (ANTs)\n",
    "    #          2) N4 bias correction (ANTs)\n",
    "    !./MONSTR.sh --t1 {t1} --t2 {t2} --o output --atlasdir TBI_atlas \n",
    "\n",
    "    # 3. Copy the relevant files (brain Mask and processed T1/T2 images)\n",
    "    createdDir = os.path.join(outputPath, os.listdir(outputPath)[0])\n",
    "\n",
    "    for file in os.listdir(createdDir):\n",
    "        if file == 'atlas4_brainmask_ANTS.nii.gz':\n",
    "            newFileName = mrn + '_bMask.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "        if file == 't1.nii.gz':\n",
    "            newFileName = mrn + '_T1GD_processed.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "        if file == 't2.nii.gz':\n",
    "            newFileName = mrn + '_T2_processed.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "    # 4. Clean the temporarily created files and directories\n",
    "    os.remove(os.path.join(workingDir, t1))\n",
    "    os.remove(os.path.join(workingDir, t2))\n",
    "    shutil.rmtree(createdDir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingDir = '/home/chansik/MONSTR'\n",
    "%cd {workingDir} \n",
    "shutil.copyfile(\"CE-T1.nii\", \"CE-T1_registered.nii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingDir = '/home/chansik/MONSTR'\n",
    "%cd {workingDir} \n",
    "\n",
    "!./MONSTR.sh --t1 CE-T1_registered.nii --t2 T2_registered.nii --fl FLAIR_registered.nii --o output --atlasdir TBI_atlas --reg --clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib.image import imsave\n",
    "#from PIL import Image\n",
    "\n",
    "workingDir = '/home/chansik/MONSTR'\n",
    "%cd {workingDir} \n",
    "\n",
    "mask_array = nib.load(\"atlas4_brainmask_ANTS.nii.gz\").get_fdata()\n",
    "    \n",
    "for file in [\"UE-T1_registered.nii\", \"CE-T1_registered.nii\", \"T2_registered.nii\", \"FLAIR_registered.nii\", \"ADC_registered.nii\"]:\n",
    "    img_array = nib.load(file).get_fdata()\n",
    "    stripped_array = img_array * mask_array\n",
    "    \n",
    "    newName = file.split(\".\")[0].split(\"_\")[0] + \"_stripped.nii\"\n",
    "    \n",
    "    niftiFile = nib.Nifti1Image(stripped_array, np.eye(4)) \n",
    "    nib.save(niftiFile, newName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"UE-T1_registered.nii\"\n",
    "file.split(\".\")[0].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    # 2. Skull stripping by MONSTR algorithm following \n",
    "    #          1) rigid coregistration of T2 onto T1 (ANTs)\n",
    "    #          2) N4 bias correction (ANTs)\n",
    "    !./MONSTR.sh --t1 {t1} --t2 {t2} --o output --atlasdir TBI_atlas \n",
    "\n",
    "    # 3. Copy the relevant files (brain Mask and processed T1/T2 images)\n",
    "    createdDir = os.path.join(outputPath, os.listdir(outputPath)[0])\n",
    "\n",
    "    for file in os.listdir(createdDir):\n",
    "        if file == 'atlas4_brainmask_ANTS.nii.gz':\n",
    "            newFileName = mrn + '_bMask.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "        if file == 't1.nii.gz':\n",
    "            newFileName = mrn + '_T1GD_processed.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "        if file == 't2.nii.gz':\n",
    "            newFileName = mrn + '_T2_processed.nii.gz'\n",
    "            shutil.copyfile(os.path.join(createdDir, file), os.path.join(targetDir, newFileName))\n",
    "\n",
    "    # 4. Clean the temporarily created files and directories\n",
    "    os.remove(os.path.join(workingDir, t1))\n",
    "    os.remove(os.path.join(workingDir, t2))\n",
    "    shutil.rmtree(createdDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # 4. MONSTR skull stripping\n",
    "    t1 = \"WB_registered.nii\"\n",
    "    t2 = \"T2_registered.nii\"\n",
    "    \n",
    "\n",
    "    # Copy the relevant files (brain Mask and processed images)\n",
    "    \n",
    "\n",
    "    shutil.copyfile(\"WB_registered.nii\", os.path.join(targetDir, patient, \"WB_registered.nii\"))\n",
    "    shutil.copyfile(\"BB_registered.nii\", os.path.join(targetDir, patient, \"BB_registered.nii\"))\n",
    "    shutil.copyfile(\"T2_registered.nii\", os.path.join(targetDir, patient, \"T2_registered.nii\"))\n",
    " \n",
    "    # 5. Clean the temporarily created files and directories\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## centering, cropping, and selecting slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function\n",
    "\n",
    "def center_crop(img, margin=10):\n",
    "    # convert an image into an sqaure image if the dimensions of x and y are different\n",
    "    if img.shape[0] != img.shape[1]:\n",
    "        if img.shape[0] < img.shape[1]:\n",
    "            pad_width = int((img.shape[1] - img.shape[0])//2)\n",
    "            img = np.pad(img, ((pad_width, pad_width), (0,0), (0,0)), constant_values=0)\n",
    "\n",
    "        if img.shape[0] > img.shape[1]:\n",
    "            pad_width = int((img.shape[0] - img.shape[1])//2)\n",
    "            img = np.pad(img, ((0,0), (pad_width, pad_width), (0,0)), constant_values=0)\n",
    "        \n",
    "    # calculate the non-zero margins of x and y axes.\n",
    "    x_min, x_max = 1000, 0\n",
    "    y_min, y_max = 1000, 0\n",
    "\n",
    "    for z in range(img.shape[2]):\n",
    "        for row in range(img.shape[0]):\n",
    "            x = np.nonzero(img[row, :, z])\n",
    "\n",
    "            if x[0].size == 0:\n",
    "                continue\n",
    "\n",
    "            if x_min > x[0].min():\n",
    "                x_min = x[0].min()\n",
    "            if x_max < x[0].max():\n",
    "                x_max = x[0].max()\n",
    "\n",
    "        for col in range(img.shape[1]):\n",
    "            y = np.nonzero(img[:, col, z])\n",
    "\n",
    "            if y[0].size == 0:\n",
    "                continue\n",
    "\n",
    "            if y_min > y[0].min():\n",
    "                y_min = y[0].min()\n",
    "            if y_max < y[0].max():\n",
    "                y_max = y[0].max()\n",
    "\n",
    "    x_width = x_max - x_min\n",
    "    x_center = x_min + (x_width//2)\n",
    "    y_width = y_max - y_min\n",
    "    y_center = y_min + (y_width//2)\n",
    "\n",
    "    if x_width >= y_width:\n",
    "        new_width = x_width + margin\n",
    "    else: \n",
    "        new_width = y_width + margin\n",
    "\n",
    "    x_start, x_end = x_center-(new_width//2), x_center+(new_width//2)\n",
    "    y_start, y_end = y_center-(new_width//2), y_center+(new_width//2)\n",
    "\n",
    "    new_img = img[y_start:y_end, x_start:x_end, :]\n",
    "    \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imsave\n",
    "from PIL import Image\n",
    "\n",
    "sourceDir = '/media/chansik/SC_research_2019/GBM-Surv/ProcessedByMONSTR'\n",
    "outputDir = '/home/chansik/DataAnalysis/201912-GBM-Surv/output-multiple'\n",
    "\n",
    "range_df = pd.read_csv('/home/chansik/DataAnalysis/201912-GBM-Surv/input/range_list.csv', header=None)\n",
    "\n",
    "target_img_size = (224, 224)\n",
    "\n",
    "for idx, row in range_df.iterrows():\n",
    "    t1_name = os.path.join(sourceDir, (str(row[0]) + \"_T1GD_processed.nii.gz\"))\n",
    "    t2_name = os.path.join(sourceDir, (str(row[0]) + \"_T2_processed.nii.gz\"))\n",
    "    mask_name = os.path.join(sourceDir, (str(row[0]) + \"_bMask.nii.gz\"))\n",
    "    \n",
    "    # Get Image Array (numpy array) from Nifti File\n",
    "    t1_array = nib.load(t1_name).get_fdata()\n",
    "    t2_array = nib.load(t2_name).get_fdata()\n",
    "    mask_array = nib.load(mask_name).get_fdata()\n",
    "    \n",
    "    t1_img = t1_array * mask_array\n",
    "    t1_img = t1_img[:, :, int(row[1]):int(row[2])]\n",
    "    \n",
    "    t2_img = t2_array * mask_array\n",
    "    t2_img = t2_img[:, :, int(row[1]):int(row[2])]\n",
    "    \n",
    "    cropped_t1 = center_crop(t1_img, margin=10) # the function 'center_crop' was defined above\n",
    "    cropped_t2 = center_crop(t2_img, margin=10)\n",
    "    \n",
    "    height = int(row[2]) - int(row[1])\n",
    "    \n",
    "    for slc in range(2, height, 5):\n",
    "        selected_t1 = cropped_t1[:, :, slc]\n",
    "        selected_t2 = cropped_t2[:, :, slc]\n",
    "        \n",
    "        newT1name = os.path.join(outputDir, (str(row[0]) + \"_T1_\" + str(slc) + \".png\"))\n",
    "        newT2name = os.path.join(outputDir, (str(row[0]) + \"_T2_\" + str(slc) + \".png\"))\n",
    "\n",
    "        imsave(newT1name, selected_t1, cmap='gray')\n",
    "        imsave(newT2name, selected_t2, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resizing and stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "baseDir = '/home/chansik/DataAnalysis/201912-GBM-Surv'\n",
    "sourceDir = os.path.join(baseDir, 'output-multiple')\n",
    "targetDir = os.path.join(baseDir, 'cnn_input')\n",
    "\n",
    "df = pd.read_csv(os.path.join(baseDir, 'input/GBM-surv-list.csv'))\n",
    "df = df.loc[df.death==1, ['ID', 'survival']]\n",
    "\n",
    "short = df.loc[df.survival < df.survival.median(), 'ID'].tolist()\n",
    "long = df.loc[df.survival >= df.survival.median(), 'ID'].tolist()\n",
    "\n",
    "short_validation_list = random.choices(short, k=30)\n",
    "long_validation_list = random.choices(long, k=30)\n",
    "\n",
    "for file in os.listdir(sourceDir):\n",
    "    \n",
    "    # patient ID number (MRN) is extracted as a variable 'mrn'\n",
    "    mrn = int(file.split('_', 1)[0])\n",
    "    \n",
    "    if mrn in short:\n",
    "        if mrn in short_validation_list:\n",
    "            shutil.copyfile(os.path.join(sourceDir, file), \n",
    "                            os.path.join(targetDir, 'validation/short', file))\n",
    "        else:\n",
    "             shutil.copyfile(os.path.join(sourceDir, file), \n",
    "                            os.path.join(targetDir, 'train/short', file))\n",
    "    \n",
    "    if mrn in long:\n",
    "        if mrn in long_validation_list:\n",
    "            shutil.copyfile(os.path.join(sourceDir, file), \n",
    "                            os.path.join(targetDir, 'validation/long', file))\n",
    "        else:\n",
    "             shutil.copyfile(os.path.join(sourceDir, file), \n",
    "                            os.path.join(targetDir, 'train/long', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(sourceDir):\n",
    "    \n",
    "    # patient ID number (MRN) is extracted as a variable 'mrn'\n",
    "    mrn = int(file.split('_', 1)[0])\n",
    "    \n",
    "    if mrn in df.ID.tolist():\n",
    "        survival = df.loc[df.ID==mrn, 'survival'].values[0]\n",
    "        new_fileName = str(survival) + \"_\" + str(mrn) + \".png\"\n",
    "\n",
    "        if mrn in validation_set:\n",
    "            shutil.copyfile(os.path.join(sourceDir, file), \n",
    "                            os.path.join(targetDir, 'validation', new_fileName))\n",
    "        else:\n",
    "            shutil.copyfile(os.path.join(sourceDir, file), \n",
    "                            os.path.join(targetDir, 'train', new_fileName))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = '/home/chansik/DataAnalysis/201912-GBM-Surv'\n",
    "sourceDir = os.path.join(baseDir, 'output')\n",
    "targetDir = os.path.join(baseDir, 'cnn_regression')\n",
    "\n",
    "[p.split('_')[os.listdir(targetDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide patients into two: short and long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "df = pd.read_csv('/home/chansik/DataAnalysis/201912-GBM-Surv/input/GBM-surv-list.csv')\n",
    "df = df.loc[df.death==1, ['ID', 'survival']]\n",
    "\n",
    "short = df.loc[df.survival < df.survival.median(), 'ID'].tolist()\n",
    "long = df.loc[df.survival >= df.survival.median(), 'ID'].tolist()\n",
    "\n",
    "sourceDir = '/home/chansik/DataAnalysis/201912-GBM-Surv/output'\n",
    "shortDir = '/home/chansik/DataAnalysis/201912-GBM-Surv/cnn_input/short'\n",
    "longDir = '/home/chansik/DataAnalysis/201912-GBM-Surv/cnn_input/long'\n",
    "\n",
    "for file in os.listdir(sourceDir):\n",
    "    \n",
    "    # patient ID number (MRN) is extracted as a variable 'mrn'\n",
    "    mrn = int(file.split('_', 1)[0])\n",
    "\n",
    "    if mrn in short:\n",
    "        shutil.copyfile(os.path.join(sourceDir, file), os.path.join(shortDir, file))\n",
    "    if mrn in long:\n",
    "        shutil.copyfile(os.path.join(sourceDir, file), os.path.join(longDir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert gray to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from matplotlib.image import imsave\n",
    "\n",
    "baseDir = '/home/chansik/DataAnalysis/201912-GBM-Surv'\n",
    "print(os.path.join(baseDir, 'output'))\n",
    "for file in os.listdir(os.path.join(baseDir, 'output')):\n",
    "    final_img_name = os.path.join(baseDir, 'gray-to-rgb', file)\n",
    "    converted = tf.image.grayscale_to_rgb(file, name=None)\n",
    "    imsave(final_img_name, converted)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "resized_image = Image.fromarray(new_img[:,:,20]).resize((224, 224))\n",
    "\n",
    "nifti_file = nib.Nifti1Image(new_img, np.eye(4))\n",
    "nib.save(nifti_file, 'test.nii') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(resized_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[:, :, 29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "new = np.pad(img, pad_width = ((1,1), (0,0), (0,0)), constant_values=0)\n",
    "print(new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imsave\n",
    "from PIL import Image\n",
    "\n",
    "image_array = nib.load(\"251020_T1GD_processed.nii.gz\").get_fdata()\n",
    "mask_array = nib.load(\"251020_bMask.nii.gz\").get_fdata()\n",
    "img = image_array * mask_array\n",
    "img = img[:, :, 53:113]\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
